{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "# Matthew Garton\n",
    "\n",
    "## NLP Project: Using Reddit's API for Predicting Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-23T19:28:02.619411Z",
     "start_time": "2017-10-23T19:28:02.600856Z"
    }
   },
   "source": [
    "In this project, I will use NLP to classify posts I have gathered using Reddit's API as coming from one of two subreddits: r/DCcomics or r/Marvel.\n",
    "\n",
    "Link to presentation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "### Scraping Thread Info from Reddit.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate needed URLS\n",
    "url_dc = 'http://www.reddit.com/r/DCcomics.json'\n",
    "url_marvel = 'https://www.reddit.com/r/Marvel.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create my header to get my requests to work\n",
    "my_header = {'User-agent': 'Matt Garton'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reddit_data(urls, header, size = 1000):\n",
    "    '''Returns a dataframe of posts from a given list of subreddits'''\n",
    "    \n",
    "    posts = []\n",
    "    after = None\n",
    "    \n",
    "    for u in urls: # iterate over urls passed to function\n",
    "        for _ in range(size): # loop the desired number of times\n",
    "            if after == None:\n",
    "                params = {}\n",
    "            else:\n",
    "                params = {'after': after}\n",
    "                \n",
    "            res = requests.get(u, params = params, headers = header) # get the data\n",
    "            \n",
    "            if res.status_code == 200: # add data to posts if request succeeds\n",
    "                the_json = res.json()\n",
    "                post = [p['data'] for p in the_json['data']['children']]\n",
    "                posts.extend(post)\n",
    "                if the_json['data']['after'] == None:\n",
    "                    continue\n",
    "                else:\n",
    "                    after = the_json['data']['after'] # update after\n",
    "            else:\n",
    "                print(res.status_code) # break and provide feedback if request fails\n",
    "                break\n",
    "                \n",
    "            time.sleep(1) # pause\n",
    "        \n",
    "    df = pd.DataFrame(posts)\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    df.to_csv('./data/reddit_data_{}{}{}_{}:{}:{}'.format(now.year, now.month, now.day, now.hour, now.minute, now.second))\n",
    "    return df\n",
    "\n",
    "comic_urls = [url_dc, url_marvel]\n",
    "\n",
    "get_reddit_data(comic_urls, my_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "Due to an error in initial attempts at data scraping, I had collected a large amount of data which was in a format I could not use. Because I had pulled the data at the wrong 'level' I was left with a dataframe where only one column was the data I needed, and each observation was a string representation of the JSON data I wanted to convert to a dataframe. In the following cells, I demonstrate the workaround I used to convert the 'bad' data into something useable, by implementing a function from Python's Abstract Syntax Trees module.\n",
    "\n",
    "# Do not run the 'old data' cells\n",
    "\n",
    "I have removed the data referenced here from this repository, due to the total amount of my data being too large for GitHub. I have left these in here to demonstrate my workflow and show the workaround I used to handle 'bad' data and make it use-able. For the purposes of testing, run the 'get data' function to get new data, and proceed as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all csv data and combine into one \n",
    "old_datasets = ['./data/dc_posts_1','./data/dc_posts_2','./data/dc_posts_3','./data/dc_posts_4',\n",
    "           './data/dc_posts_5','./data/dc_posts_6','./data/marvel_posts']\n",
    "\n",
    "old_data = [pd.read_csv(d) for d in old_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each dataframe in the list I created, convert the string in the 'data' column\n",
    "# into a dictionary, then convert the result into a dataframe\n",
    "# finally, put all dataframes into a list\n",
    "# when I tried to do the below in one for loop over all dataframes, my Jupyter Notebook crashed\n",
    "# so I did the 'brute force' approach below\n",
    "\n",
    "data_1 = pd.DataFrame([ast.literal_eval(d) for d in old_data[0]['data']])\n",
    "data_2 = pd.DataFrame([ast.literal_eval(d) for d in old_data[1]['data']])\n",
    "data_3 = pd.DataFrame([ast.literal_eval(d) for d in old_data[2]['data']])\n",
    "data_4 = pd.DataFrame([ast.literal_eval(d) for d in old_data[3]['data']])\n",
    "data_5 = pd.DataFrame([ast.literal_eval(d) for d in old_data[4]['data']])\n",
    "data_6 = pd.DataFrame([ast.literal_eval(d) for d in old_data[5]['data']])\n",
    "data_7 = pd.DataFrame([ast.literal_eval(d) for d in old_data[6]['data']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data = pd.concat([data_1, data_2, data_3, data_4, data_5, data_6]) # concatenate the results into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data.drop_duplicates(subset = 'id', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data.dropna(subset = ['selftext'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found myself repeating the steps above a number of times, so I decided to write a function for my initial data cleaning. Run this cell to ensure the function is in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_reddit_data(df):\n",
    "    '''Returns a cleaned dataframe with duplicates and nulls removed'''\n",
    "    \n",
    "    df.drop_duplicates(subset = 'id', inplace = True) # remove instances where the same post was pulled twice\n",
    "    df.dropna(subset = ['selftext'], inplace = True) # remove observations which contain no data\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: Do not run the two cells below. \n",
    "\n",
    "Again, these are here to demonstrate my workflow. For the purposes of testing my models, it is necessary to get new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in good datasets and concat to a dataframe\n",
    "\n",
    "good_datasets = ['./data/reddit_data_201897_10:14:21', './data/reddit_data_201897_10:35:26',\n",
    "                 './data/reddit_data_201897_11:10:35', './data/reddit_data_201897_11:34:3', \n",
    "                 './data/reddit_data_201897_14:34:56', './data/reddit_data_201898_14:24:10',\n",
    "                './data/reddit_data_201897_15:9:56', './data/reddit_data_201899_9:37:39',\n",
    "                './data/reddit_data_201899_9:46:17']\n",
    "\n",
    "good_data = pd.concat([pd.read_csv(d, index_col = 0) for d in good_datasets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = clean_reddit_data(good_data) # clean the data\n",
    "\n",
    "# combine old and new data - repeat cleaning process for good measure\n",
    "data_full = pd.concat([old_data, data_clean]) \n",
    "\n",
    "reddit_data = clean_reddit_data(data_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Part 1\n",
    "\n",
    "I now believe I have a sufficiently sized dataset (this is the most I can get at this point without getting archives). Time for some basic EDA, starting with: what is in each column? How balanced is my data? What is the text data that I will use for my predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data['subreddit'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Accuracy\n",
    "\n",
    "Right off the bat, I have much more DC data than Marvel Data. I am not sure what is driving this, but I want to try to get more Marvel Data to correct this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reddit_data['subreddit'].value_counts(normalize = True))\n",
    "print('')\n",
    "print(reddit_data['subreddit'].value_counts(normalize = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-sample (w/o replacement) for new marvel data to balance out classes.\n",
    "\n",
    "urls = [url_marvel]\n",
    "new_marvel = get_reddit_data(urls, header = my_header, size = 1000)\n",
    "new_marvel = clean_reddit_data(new_marvel)\n",
    "reddit_data = pd.concat([reddit_data, new_marvel])\n",
    "reddit_data = clean_reddit_data(reddit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reddit_data['subreddit'].value_counts(normalize = True))\n",
    "print('')\n",
    "print(reddit_data['subreddit'].value_counts(normalize = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have more balanced data to work with. Export to csv to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data.to_csv('./data/reddit_data_from_notebook_3', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull in full dataset from last check point, rather than going through full data gathering/cleaning process again.\n",
    "# WARNING: I ran into errors when I tried to follow my workflow after pulling in the data from the saved csv. \n",
    "\n",
    "I need to look into this, but for now, I am skipping this step and using the data I have gathered above. The biggest problem with this is that, for some reason, I need to re-sample Marvel data each time I want to build a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data = pd.read_csv('./data/reddit_data_from_notebook_3')\n",
    "reddit_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data for Modeling\n",
    "\n",
    "In order to prepare my text data for modeling, I need to take the following steps:\n",
    "\n",
    "1. Create my X variable - I will create a 'document' for each row containing the selftext and title of each post\n",
    "2. Create my Y variable - I will create a binary variable from the 'subreddit' column, indicating 1 for r/DCcomics and 1 for r/Marvel.\n",
    "3. Preprocess text - convert the long string of my data into a list of lemmatized words, all lowercase with no punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining title and selftext into one column which represents 'data'\n",
    "reddit_data['data'] = reddit_data['selftext'] + ' ' + reddit_data['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index of df\n",
    "reddit_data.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text\n",
    "\n",
    "# import necessary modules\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+') \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#function to preprocess text\n",
    "def preprocess_text(string):\n",
    "    '''Generalized function to preprocess text data'''\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", string) # remove punctuation\n",
    "    letters_lower = letters_only.lower() # make lowercase\n",
    "    words = letters_lower.split() # split into words\n",
    "    lemmas = [lemmatizer.lemmatize(w) for w in words]  # lemmatize\n",
    "    return (' '.join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data['data'] = reddit_data['data'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set y variable as dummy for subreddit\n",
    "reddit_data = pd.get_dummies(reddit_data, columns = ['subreddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = reddit_data['data']\n",
    "y = reddit_data['subreddit_DCcomics']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, random_state = 43, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "db045898-1d2d-4af2-8e79-437c4c7546b4"
   },
   "source": [
    "## NLP\n",
    "\n",
    "#### Use `CountVectorizer` or `TfidfVectorizer` from scikit-learn to create features from the thread titles and descriptions (NOTE: Not all threads have a description)\n",
    "- Examine using count or binary features in the model\n",
    "- Re-evaluate your models using these. Does this improve the model performance? \n",
    "- What text features are the most valuable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "## Count Vectorizer First - remove stopwords\n",
    "cvec = CountVectorizer(stop_words = 'english') # Instantiate\n",
    "cvec = cvec.fit(X_train) # Fit\n",
    "\n",
    "# Transform train and test\n",
    "\n",
    "cvec_train = cvec.transform(X_train)\n",
    "cvec_test = cvec.transform(X_test)\n",
    "\n",
    "# Tfidf Vectorizer - remove stopwords\n",
    "tfidf = TfidfVectorizer(stop_words = 'english') # Instantiate\n",
    "tfidf = tfidf.fit(X_train)\n",
    "\n",
    "# Transform train and test\n",
    "\n",
    "tfidf_train = tfidf.transform(X_train)\n",
    "tfidf_test = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Part 2 - Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = cvec.get_feature_names()\n",
    "dense_cvec = cvec_train.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(dense_cvec, columns = words)\n",
    "words_df.head()\n",
    "\n",
    "word_counts = words_df.apply(sum)\n",
    "words_transpose = words_df.T\n",
    "words_transpose['count'] = word_counts\n",
    "\n",
    "words_transpose[['count']].sort_values(by = 'count', ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the most frequent words reveals another problem with my data - words like 'http' and 'www' are not meaningful text. I would want to figure out how to remove these from my data to improve upon my model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "04563b69-f7b6-466f-9d65-fc62c9ddee6a"
   },
   "source": [
    "## Predicting subreddit using Random Forests + Another Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "588f9845-6143-4bcc-bfd1-85d45b79303d"
   },
   "outputs": [],
   "source": [
    "# import necessary modules for modeling\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a7afb2c0-d41e-4779-8216-91cd8dd4473f"
   },
   "source": [
    "#### Thought experiment: What is the baseline accuracy for this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "87a17d3d-b7f4-4747-9f75-f9af1d18a174"
   },
   "source": [
    "As discovered earlier, the baseline accuracy for this model is the proportion of my data that is in the majority class. In this case, about 59%, since 59% of my data came from the DCcomics subreddit. \n",
    "\n",
    "Note again that my initial dataset was heavily unbalanced, with over 80% of observations coming from DCcomics. To address this, I simply resampled Marvel data. Since my goal here was to get as many datapoints from each subreddit as possible, I do not think that doing so is bad practice. If it weren't for Reddit's restrictions, I would have pulled an exact 50/50 split.\n",
    "\n",
    "It is possible that the reason I initially got more DC data than Marvel data is that a larger amount of Marvel data was lost in the data cleaning process. If that is the case, then the lack of balance would actually tell me something about the difference between the two populations, and my assumption above would not be valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "4fb29de2-5b98-474c-a4ad-5170b72b9aea"
   },
   "source": [
    "#### Create a `RandomForestClassifier` model to predict which subreddit a given post belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "ddbc6159-6854-4ca7-857f-bfecdaf6d9c2"
   },
   "outputs": [],
   "source": [
    "## RandomForestClassifier - Count Vectorizer\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf = rf.fit(cvec_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(cvec_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.n_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def evaluate_model(model, test_data, test_target):\n",
    "    '''Prints out an evaluation of a given model, including:\n",
    "    Accuracy, Confusion Matrix and a ROC-AUC curve'''\n",
    "    accuracy = model.score(test_data, test_target) # Calculate accuracy\n",
    "    predictions = model.predict(test_data) # Use model to predict class\n",
    "    \n",
    "    cm = confusion_matrix(y_test, predictions) # Create confusion matrix\n",
    "    cm_df = pd.DataFrame(data=cm, columns=['predicted Marvel', 'predicted DC'], \n",
    "                         index=['actual Marvel', 'actual DC'])\n",
    "    print('Model Accuracy: {}\\n'.format(accuracy))\n",
    "    return cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(rf, cvec_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RandomForestClassifier - Tfidf Vectorizer\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(tfidf_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(rf, cvec_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = rf.feature_importances_\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "9367beff-72ba-4768-a0ba-a50b335de61d"
   },
   "source": [
    "#### Use cross-validation in scikit-learn to evaluate the model above. \n",
    "- Evaluate the accuracy of the model, as well as any other metrics you feel are appropriate. \n",
    "- **Bonus**: Use `GridSearchCV` with `Pipeline` to optimize your `CountVectorizer`/`TfidfVectorizer` and classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat the model-building process using a different classifier (e.g. `MultinomialNB`, `LogisticRegression`, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb = mnb.fit(cvec_train, y_train)\n",
    "evaluate_model(mnb, cvec_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr = lr.fit(cvec_train, y_train)\n",
    "evaluate_model(lr, cvec_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn = knn.fit(cvec_train, y_train)\n",
    "evaluate_model(knn, cvec_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb = mnb.fit(tfidf_train, y_train)\n",
    "evaluate_model(mnb, cvec_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr = lr.fit(tfidf_train, y_train)\n",
    "evaluate_model(lr, cvec_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn = knn.fit(tfidf_train, y_train)\n",
    "evaluate_model(knn, cvec_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutations of Models to Test\n",
    "\n",
    "I will follow the below framework when I return to this project to clean it up and finish the research.\n",
    "\n",
    "## Vectorizers\n",
    "\n",
    "1. CountVectorizer (binary - yes or no; n-gram range)\n",
    "2. TfidfVectorizer\n",
    "\n",
    "## Models\n",
    "\n",
    "1. RandomForestClassifier\n",
    "2. KNNClassifier(n_neighbors; distance; weights)\n",
    "3. LogisticRegression\n",
    "4. MultinomialNB\n",
    "5. SupportVectorMachines\n",
    "\n",
    "\n",
    "# Output\n",
    "\n",
    "## For each model, produce:\n",
    "\n",
    "1. Score\n",
    "2. Confusion Matrix\n",
    "3. ROC-AUC curve\n",
    "4. Best Params\n",
    "5. Best Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "steps = [\n",
    "    (\"vectorizer\", CountVectorizer()),\n",
    "    (\"knn\", KNeighborsClassifier())\n",
    "]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "grid_params = {\n",
    "    \"knn__n_neighbors\": [3,10,20],\n",
    "    \"knn__weights\": [\"distance\", \"uniform\"]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, grid_params, verbose=2)\n",
    "results = gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "---\n",
    "Put your executive summary in a Markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The data science problem I attempted to address in this project was: _What characteristics of a post on Reddit contribute most to what subreddit it belongs to?_. My approach was to collect as many posts as possible from two subreddits using Reddit's API, and then to apply NLP and classification models to that data. My goals were the following: 1) Build the best prediction model possible, 2) Learn what characteristics were most useful in predicting a subreddit, 3) Automate and streamline my workflow as much as possible by building functions to handle repeated tasks, 4) Learn how to leverage Pipelines and Gridsearch, as well as review the various models I have learned thus far.\n",
    "\n",
    "    The two subreddits I chose to use for this analysis were r/DCcomics and r/Marvel. I thought it might be a relatively eady problem for a human (with enough subject matter knowledge) to predict between the two and I wondered if the same was true for a machine learning algorithm. I also wanted to explore which features would be most useful in predicting between the two - are they the things I would expect to be most obvious, such as character names? Additionally, I am interested in looking at how a model would change if the 'proper noun' features were removed.\n",
    "    \n",
    "    The major problem encountered in this project was in data gathering. First, reddit only retains 1000 of the most recent posts, so there is an upper limit on the amount of data that can be obtained without learning how to access the archived. Compounding the problem was the fact that my function did not 'break' as I would expect when reaching the end of the available posts - it looped back up to the beginning Looking through the posts on each subreddit, I noticed this could be a problem, as many of the posts did not contain text data at all - they were just pictures. I think this could be a unique problem with using comic book subreddits, but I am not sure. \n",
    "    \n",
    "    The second problem I encountered with my data is that in my initial data pulls, I was saving the data to a dataframe at the wrong level, so I needed to figure out how to extract the data that I needed (this was necessary because I could not get enough 'new' data, so I needed to access the 'old' data. I used a Python module called Abstract Syntax Trees in order to accomplish this.\n",
    "    \n",
    "    Once I had all of the data that I needed, I began building prediction models. I used both Count Vectorizer and tf-idf Vectorizers to transform my text data, and tested several models: Random Forests, Multinomial Naive Bayes, Logistic Regression, and K Nearest Neighbors. All models improved on the 'Baseline Accuracy', but Logistic Regression with Count Vectorized data was the best model, with an accuracy of ~91%. \n",
    "    \n",
    "    The primary lesson I learned from this project is to carefully examine data as I am collecting it to catch problems early. In terms of modeling, I was ultimately unable to do the modeling practice that I was hoping to accomplish with this project, due to time constraints, so I did not learn much in terms of model building or hyperparameter tuning.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
